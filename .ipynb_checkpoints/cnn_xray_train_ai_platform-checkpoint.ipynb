{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35238266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2059f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-02-15ad15b6da61\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59f6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-02-15ad15b6da61\"\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-east1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f70fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65be199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1de2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a025f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cb06f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=512\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[128, 32, 4]\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=30\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
    "        If this is more than actual # of examples available, it cycles through\n",
    "        them. So specifying 1000 here when you have only 100k examples makes\n",
    "        this 10 epochs.\"\"\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "  \n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5ed32c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "def load_data(train_path, val_path, batch_size):\n",
    "    \n",
    "    CLASS_LABELS = ['NORMAL', 'PNEUMONIA'] \n",
    "\n",
    "    def process_path(nb_class):\n",
    "    \n",
    "        def f(file_path):\n",
    "            \n",
    "            label = 0 if tf.strings.split(file_path, os.path.sep)[-2]=='NORMAL' else 1\n",
    "            \n",
    "            image = tf.io.read_file(file_path)    \n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "         \n",
    "            image = tf.image.resize(image, [127, 127], method='area')\n",
    "            return image, label\n",
    "    \n",
    "        return f\n",
    "\n",
    "    def reader_image(path_file, batch_size, nb_class):\n",
    "\n",
    "        list_ds = tf.data.Dataset.list_files(path_file)\n",
    "        labeled_ds = list_ds.map(process_path(nb_class))\n",
    "    \n",
    "        return labeled_ds.shuffle(100).batch(batch_size).prefetch(1)\n",
    "    \n",
    "    train_ds = reader_image(train_path, batch_size, 2)\n",
    "    val_ds = reader_image(val_path, batch_size, 2)\n",
    "\n",
    "    print(type(train_ds))\n",
    "\n",
    "\n",
    "    for image, label in train_ds.take(1):\n",
    "        df = pd.DataFrame(image[0, :, :, 0].numpy())\n",
    "    \n",
    "    print(f'Outoupt : \\n image shape: {df.shape}')\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def train_and_evaluate(args):  \n",
    "\n",
    "    num_classes = 2\n",
    "    cnn_model = tf.keras.Sequential([\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 3, activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    optm = Adam(lr=0.0001)\n",
    "    cnn_model.compile(loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                      optimizer=optm, \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/pneumonia\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "    \n",
    "    train_ds, val_ds = load_data(args[\"train_data_path\"], args[\"eval_data_path\"], args[\"batch_size\"])\n",
    "    \n",
    "    model_history = cnn_model.fit(\n",
    "              train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=args[\"num_epochs\"])\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=cnn_model, export_dir=EXPORT_PATH)\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42d5e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "Outoupt : \n",
      " image shape: (127, 127)\n",
      "348/348 [==============================] - 254s 715ms/step - loss: 0.5824 - accuracy: 0.7416 - val_loss: 0.7971 - val_accuracy: 0.5000\n",
      "Exported trained model to xray_trained/20210624050819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 05:03:51.691096: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\n",
      "2021-06-24 05:03:51.691156: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\n",
      "2021-06-24 05:03:51.691166: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\n",
      "2021-06-24 05:03:51.691173: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\n",
      "2021-06-24 05:03:53.104780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 05:03:53.105663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 05:03:53.116067: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-24 05:03:53.117446: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1728] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-06-24 05:03:53.117758: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-24 05:03:57.886221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-24 05:08:20.006552: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=xray_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/val/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=15 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb7950b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: xray_cnn_210624_051541\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [xray_cnn_210624_051541] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe xray_cnn_210624_051541\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs xray_cnn_210624_051541\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/xray_models/trained_model\n",
    "JOBID=xray_cnn_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train/*/*.jpeg \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/test/*/*.jpeg \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=1 \\\n",
    "    --batch_size=32 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bf40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
