{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4150806b",
   "metadata": {},
   "source": [
    "## Training on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60551c",
   "metadata": {},
   "source": [
    "Since we have a large number of files, we need to load the data using a csv generator rather than copy all the data to the VM. For this we will need to do preprocessing. We need to create a CSV file which has the path and the lab as two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for c in $(gsutil ls gs://open_project/chest_xray/test/NORMAL)\n",
    "do echo $c,NORMAL;\n",
    "done >> labels.csv;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for c in $(gsutil ls gs://open_project/chest_xray/test/PNEUMONIA)\n",
    "do echo $c,PNEUMONIA;\n",
    "done >> labels.csv;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b0ac2",
   "metadata": {},
   "source": [
    "Split the training data to 80% train and 20% validation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af9f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/jupyter/open-project-chest-x-rays /labels.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "train = df.sample(frac=0.8, random_state=123)\n",
    "validation = df.loc[~df.index.isin(train.index)]\n",
    "train.to_csv('train.csv', index=False)\n",
    "validation.to_csv('validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e62fe19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][326.6 KiB/326.6 KiB]                                                \n",
      "Operation completed over 1 objects/326.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp train.csv gs://open_project/chest_xray/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2555a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://validation.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 81.9 KiB/ 81.9 KiB]                                                \n",
      "Operation completed over 1 objects/81.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp validation.csv gs://open_project/chest_xray/val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7df78",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7765db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training csv file\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation csv file\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"Number of epochs to train the model.\",\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
    "        to None, which means to evaluate until input_fn raises an end-of-input\n",
    "        exception\"\"\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "#     arguments[\"output_dir\"] = os.path.join(\n",
    "#         arguments[\"output_dir\"],\n",
    "#         json.loads(\n",
    "#             os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "#         ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "#     )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dd518ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "CLASS_NAMES = ['NORMAL','PNEUMONIA']\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "VALIDATION_IMAGES = 370\n",
    "\n",
    "def decode_img(img, reshape_dims):\n",
    "    # Convert the compressed string to a 3D uint8 tensor.\n",
    "    img = tf.image.decode_jpeg(img, channels=IMG_CHANNELS)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # Resize the image to the desired size.\n",
    "    return tf.image.resize(img, reshape_dims)\n",
    "\n",
    "\n",
    "def decode_csv(csv_row):\n",
    "    record_defaults = [\"path\", \"label\"]\n",
    "    filename, label_string = tf.io.decode_csv(csv_row, record_defaults)\n",
    "    image_bytes = tf.io.read_file(filename=filename)\n",
    "    label = tf.math.equal(CLASS_NAMES, label_string)\n",
    "    return image_bytes, label\n",
    "\n",
    "def read_and_preprocess(image_bytes, label, random_augment=False):\n",
    "    img = decode_img(image_bytes, [IMG_WIDTH, IMG_HEIGHT])\n",
    "    return img, label\n",
    "\n",
    "def load_dataset(csv_of_filenames, batch_size, training=True):\n",
    "    dataset = tf.data.TextLineDataset(filenames=csv_of_filenames) \\\n",
    "        .map(decode_csv).cache()\n",
    "\n",
    "    if training:\n",
    "        dataset = dataset \\\n",
    "            .map(read_and_preprocess) \\\n",
    "            .shuffle(10*batch_size) \\\n",
    "            .repeat(count=None)  # Indefinately.\n",
    "    else:\n",
    "        dataset = dataset \\\n",
    "            .map(read_and_preprocess) \\\n",
    "            .repeat(count=1)  # Each photo used once.\n",
    "\n",
    "    # Prefetch prepares the next set of batches while current batch is in use.\n",
    "    return dataset.batch(batch_size=batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "def build_model():\n",
    "    num_classes = 2\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        Flatten(),\n",
    "        Dense(num_classes),\n",
    "        Softmax()\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer='adam',\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy']) #since the last layer is a softmax we can only measure accuracy\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    model = build_model()\n",
    "    train_ds = load_dataset(\n",
    "        args[\"train_data_path\"],\n",
    "        args[\"batch_size\"])\n",
    "\n",
    "    eval_ds = load_dataset(\n",
    "        args[\"eval_data_path\"], args[\"batch_size\"], training=False)\n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/pneumonia\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "    history =     model.fit(\n",
    "                          train_ds,\n",
    "                          validation_data=eval_ds,\n",
    "                          steps_per_epoch=5,\n",
    "                          epochs=args[\"num_epochs\"],\n",
    "                          validation_steps=2,\n",
    "#                           callbacks=[cp_callback]\n",
    "                        )\n",
    "\n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "#     hp_metric = history.history['val_rmse'][-1]\n",
    "\n",
    "#     hpt = hypertune.HyperTune()\n",
    "#     hpt.report_hyperparameter_tuning_metric(\n",
    "#         hyperparameter_metric_tag='rmse',\n",
    "#         metric_value=hp_metric,\n",
    "#         global_step=args['num_epochs'])\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8021746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 11s 1s/step - loss: 0.6148 - accuracy: 0.7375 - val_loss: 0.5622 - val_accuracy: 0.7656\n",
      "Exported trained model to pneumonia_trained/20210624013539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 01:35:27.078838: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\n",
      "2021-06-24 01:35:27.078899: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\n",
      "2021-06-24 01:35:27.078907: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\n",
      "2021-06-24 01:35:27.078914: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\n",
      "2021-06-24 01:35:28.415218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 01:35:28.416196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-24 01:35:28.425814: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-24 01:35:28.426965: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1728] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-06-24 01:35:28.427256: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-24 01:35:29.039630: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:164] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-24 01:35:39.450662: W tensorflow/core/kernels/data/cache_dataset_ops.cc:764] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-24 01:35:39.584295: W tensorflow/core/kernels/data/cache_dataset_ops.cc:764] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-24 01:35:39.762846: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "BUCKET='open_project'\n",
    "OUTDIR=pneumonia_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train.csv  \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/val.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "538249b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"qwiklabs-gcp-00-8478c1b0a0c1\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT   # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd36fdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: pneumonia_210624_014829\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [pneumonia_210624_014829] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe pneumonia_210624_014829\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs pneumonia_210624_014829\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "BUCKET='open_project'\n",
    "OUTDIR=gs://open_project/chest_xray/trained_model\n",
    "JOBID=pneumonia_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/chest_xray/train.csv  \\\n",
    "    --eval_data_path=gs://${BUCKET}/chest_xray/val.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --eval_steps=100 \\\n",
    "    --batch_size=32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694aaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
